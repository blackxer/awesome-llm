# 论文资源

#### 模型训练
- [ ] Stack More Layers Differently:High-Rank Training Through Low-Rank Updates (ReLoRA)【[paper](https://arxiv.org/abs/2307.05695)】【[code](https://github.com/guitaricet/peft_pretraining)】【2023】
- [x] MEASURING THE INTRINSIC DIMENSION OF OBJECTIVE LANDSCAPES 【[论文](https://arxiv.org/pdf/1804.08838.pdf)】
- [x] INTRINSIC DIMENSIONALITY EXPLAINS THE EFFECTIVENESS OF LANGUAGE MODEL FINE-TUNING 【[论文](https://arxiv.org/pdf/2012.13255.pdf)】
- [x] LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS 【[论文](https://arxiv.org/pdf/2106.09685.pdf)】【[代码](https://github.com/microsoft/LoRA)】
- [x] QLORA: Efficient Finetuning of Quantized LLMs 【[论文](https://arxiv.org/pdf/2305.14314v1.pdf)】【[代码](https://github.com/artidoro/qlora)】
- [x] llama【[论文](https://arxiv.org/pdf/2302.13971v1.pdf)】【[代码](https://github.com/facebookresearch/llama/tree/llama_v1)】
- [x] llama2 【[论文](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)】【[代码](https://github.com/facebookresearch/llama)】
- [x] opt 训练日志编年史【[Chronicles ](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/README.md)】


#### 思维链推理
- [ ] Graph of Thoughts: Solving Elaborate Problems with Large Language Models (GoT)【[paper](https://arxiv.org/pdf/2308.09687v2.pdf)】【[code](https://github.com/spcl/graph-of-thoughts)】【2023】
- [ ] Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework【[paper](https://arxiv.org/abs/2305.03268)】【[code](https://github.com/RuochenZhao/Verify-and-Edit)】【2023】
- [x] Tree of thoughts: Deliberate problem solving with large language models (ToT)【[paper](https://arxiv.org/pdf/2305.10601.pdf)】【[code](https://github.com/princeton-nlp/tree-of-thought-llm)】【2023】
- [x] Large language model guided tree-of-thought (ToT)【[paper](https://arxiv.org/pdf/2305.08291.pdf)】【[code](https://github.com/jieyilong/tree-of-thought-puzzle-solver)】【2023】
- [x] React: Synergizing reasoning and acting in language models【[paper](https://arxiv.org/abs/2210.03629)】【[code](https://github.com/ysymyth/ReAct)】【2023】
- [x] Reflexion: an autonomous agent with dynamic memory and self-reflection【[paper](https://arxiv.org/pdf/2303.11366.pdf)】【[code](https://github.com/noahshinn024/reflexion)】【2023】
- [ ] Automatic prompt augmentation and selection with chain-of-thought from labeled data【2023】
- [x] Decomposition enhances reasoning via self-evaluation guided decoding【[paper](https://arxiv.org/pdf/2305.00633.pdf)】【[code](https://github.com/YuxiXie/SelfEval-Guided-Decoding)】【2023】
- [x]  Self-consistency improves chain of thought reasoning in language models (CoT-SC)【[paper](https://arxiv.org/pdf/2203.11171.pdf)】【2022】
- [x] Chain of thought prompting elicits reasoning in large language models (CoT)【[paper](https://arxiv.org/pdf/2201.11903v6.pdf)】【2022】


#### 指令数据集生成
- [x] Large language models are human-level prompt engineers【[paper](https://arxiv.org/pdf/2211.01910.pdf)】【[code](https://github.com/keirp/automatic_prompt_engineer)】【2023】
