# è®ºæ–‡èµ„æº

#### å¤§æ¨¡å‹è®­ç»ƒ
- [ ] 2x Faster Language Model Pre-training via Masked Structural Growthã€[paper](https://arxiv.org/abs/2305.02869)ã€‘ã€2023ã€‘
- [ ] Freelm: Fine-tuning-free language modelã€[paper](https://arxiv.org/abs/2305.01616)ã€‘ã€2023ã€‘
- [ ] GPT-4 technical reportã€[paper](https://arxiv.org/abs/2303.08774)ã€‘ã€2023ã€‘
- [ ] Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scalesã€[paper](https://arxiv.org/abs/2304.06875)ã€‘ã€[code](https://github.com/cofe-ai/Mu-scaling)ã€‘ã€2023ã€‘
- [x] FLM-101B: An Open LLM and How to Train It with $100K Budgetã€[paper](https://arxiv.org/pdf/2309.03852.pdf)ã€‘ã€[code](https://huggingface.co/CofeAI/FLM-101B)ã€‘ã€2023ã€‘
- [x] Baichuan2ã€[paper](https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf)ã€‘ã€[code](https://github.com/baichuan-inc/Baichuan2)ã€‘ã€2023ã€‘
- [x] llama2 ã€[è®ºæ–‡](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)ã€‘ã€[ä»£ç ](https://github.com/facebookresearch/llama)ã€‘ã€2023ã€‘
- [x] llamaã€[è®ºæ–‡](https://arxiv.org/pdf/2302.13971v1.pdf)ã€‘ã€[ä»£ç ](https://github.com/facebookresearch/llama/tree/llama_v1)ã€‘ã€2023ã€‘
- [x] opt è®­ç»ƒæ—¥å¿—ç¼–å¹´å²ã€[Chronicles ](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/README.md)ã€‘ã€2022ã€‘
- [ ] Training Compute-Optimal Large Language Modelsã€[paper](https://arxiv.org/abs/2203.15556)ã€‘ã€2022ã€‘
- [ ] Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transferã€[paper](https://arxiv.org/abs/2203.03466)ã€‘ã€[code](https://github.com/microsoft/mup)ã€‘ã€2022ã€‘
- [ ] Scaling Laws for Neural Language Modelsã€[paper](https://arxiv.org/abs/2001.08361)ã€‘ã€2020ã€‘
- [ ] Scaling Laws for Autoregressive Generative Modelingã€[paper](https://arxiv.org/abs/2010.14701)ã€‘ã€2020ã€‘


#### å¤§æ¨¡å‹é‡Œçš„å°æ¨¡å‹ğŸ˜‚
- [ ] Textbooks Are All You Need II: phi-1.5 technical reportã€[paper](https://arxiv.org/abs/2309.05463)ã€‘ã€ã€‘ã€2023ã€‘
- [ ] TinyLlama-1.1Bã€[code](https://github.com/jzhang38/TinyLlama)ã€‘ã€2023ã€‘


#### ä½ç½®ç¼–ç 
- [ ] YaRN: Efficient Context Window Extension of Large Language Modelsã€[paper](https://arxiv.org/abs/2309.00071)ã€‘ã€[code](https://github.com/jquesnelle/yarn)ã€‘ã€2023ã€‘
- [ ] A length-extrapolatable transformerã€[paper](https://arxiv.org/pdf/2212.10554.pdf)ã€‘ã€[code](https://github.com/sunyt32/torchscale)ã€‘ã€2022ã€‘
- [ ] RoFormer: Enhanced Transformer with Rotary Position Embeddingã€[paper](https://arxiv.org/abs/2104.09864)ã€‘ã€[code](https://github.com/ZhuiyiTechnology/roformer)ã€‘ã€2021ã€‘
- [ ] Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation ã€[paper](https://arxiv.org/abs/2108.12409)ã€‘ã€[code](https://github.com/ofirpress/attention_with_linear_biases)ã€‘ã€2021ã€‘


#### è½»é‡å¾®è°ƒ
- [ ] Memory Efficient Optimizers with 4-bit Statesã€[paper](https://arxiv.org/abs/2309.01507)ã€‘ã€[code]( https://github.com/thu-ml/low-bit-optimizers)ã€‘ã€2023ã€‘
- [x] Stack More Layers Differently:High-Rank Training Through Low-Rank Updates (ReLoRA)ã€[paper](https://arxiv.org/abs/2307.05695)ã€‘ã€[code](https://github.com/guitaricet/peft_pretraining)ã€‘ã€2023ã€‘
- [x] QLORA: Efficient Finetuning of Quantized LLMs ã€[è®ºæ–‡](https://arxiv.org/pdf/2305.14314v1.pdf)ã€‘ã€[ä»£ç ](https://github.com/artidoro/qlora)ã€‘ã€2023ã€‘
- [x] LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS ã€[è®ºæ–‡](https://arxiv.org/pdf/2106.09685.pdf)ã€‘ã€[ä»£ç ](https://github.com/microsoft/LoRA)ã€‘ã€2021ã€‘
- [x] INTRINSIC DIMENSIONALITY EXPLAINS THE EFFECTIVENESS OF LANGUAGE MODEL FINE-TUNING ã€[è®ºæ–‡](https://arxiv.org/pdf/2012.13255.pdf)ã€‘ã€2020ã€‘
- [x] MEASURING THE INTRINSIC DIMENSION OF OBJECTIVE LANDSCAPES ã€[è®ºæ–‡](https://arxiv.org/pdf/1804.08838.pdf)ã€‘ã€2018ã€‘


#### æ€ç»´é“¾æ¨ç†
- [ ] Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models (AoT)ã€[paper](https://arxiv.org/pdf/2308.10379.pdf)ã€‘ã€ã€‘ã€2023ã€‘
- [ ] Large Language Models as Optimizers ã€[paper](https://arxiv.org/abs/2309.03409)ã€‘ã€2023ã€‘
- [ ] Graph of Thoughts: Solving Elaborate Problems with Large Language Models (GoT)ã€[paper](https://arxiv.org/pdf/2308.09687v2.pdf)ã€‘ã€[code](https://github.com/spcl/graph-of-thoughts)ã€‘ã€2023ã€‘
- [x] Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Frameworkã€[paper](https://arxiv.org/abs/2305.03268)ã€‘ã€[code](https://github.com/RuochenZhao/Verify-and-Edit)ã€‘ã€2023ã€‘
- [x] Tree of thoughts: Deliberate problem solving with large language models (ToT)ã€[paper](https://arxiv.org/pdf/2305.10601.pdf)ã€‘ã€[code](https://github.com/princeton-nlp/tree-of-thought-llm)ã€‘ã€2023ã€‘
- [x] Large language model guided tree-of-thought (ToT)ã€[paper](https://arxiv.org/pdf/2305.08291.pdf)ã€‘ã€[code](https://github.com/jieyilong/tree-of-thought-puzzle-solver)ã€‘ã€2023ã€‘
- [x] React: Synergizing reasoning and acting in language modelsã€[paper](https://arxiv.org/abs/2210.03629)ã€‘ã€[code](https://github.com/ysymyth/ReAct)ã€‘ã€2023ã€‘
- [x] Reflexion: an autonomous agent with dynamic memory and self-reflectionã€[paper](https://arxiv.org/pdf/2303.11366.pdf)ã€‘ã€[code](https://github.com/noahshinn024/reflexion)ã€‘ã€2023ã€‘
- [ ] Automatic prompt augmentation and selection with chain-of-thought from labeled dataã€2023ã€‘
- [x] Decomposition enhances reasoning via self-evaluation guided decodingã€[paper](https://arxiv.org/pdf/2305.00633.pdf)ã€‘ã€[code](https://github.com/YuxiXie/SelfEval-Guided-Decoding)ã€‘ã€2023ã€‘
- [x]  Self-consistency improves chain of thought reasoning in language models (CoT-SC)ã€[paper](https://arxiv.org/pdf/2203.11171.pdf)ã€‘ã€2022ã€‘
- [x] Chain of thought prompting elicits reasoning in large language models (CoT)ã€[paper](https://arxiv.org/pdf/2201.11903v6.pdf)ã€‘ã€2022ã€‘


#### æŒ‡ä»¤æ•°æ®é›†ç”Ÿæˆ
- [x] Large language models are human-level prompt engineersã€[paper](https://arxiv.org/pdf/2211.01910.pdf)ã€‘ã€[code](https://github.com/keirp/automatic_prompt_engineer)ã€‘ã€2023ã€‘
- [x] Self-Alignment with Instruction Backtranslationã€[paper](https://arxiv.org/abs/2308.06259)ã€‘ã€2023ã€‘
- [ ] WizardLM: Empowering Large Language Models to Follow Complex Instructionsã€[paper](https://arxiv.org/abs/2304.12244)ã€‘ã€2023ã€‘
- [ ] LongForm: Optimizing Instruction Tuning for Long Text Generation with Corpus Extractionã€[paper](https://arxiv.org/abs/2304.08460)ã€‘ã€2023ã€‘
- [ ] Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervisionã€[paper](https://arxiv.org/abs/2305.03047)ã€‘ã€2023ã€‘
- [ ] LIMA: Less Is More for Alignmentã€[paper](https://arxiv.org/abs/2305.11206)ã€‘ã€2023ã€‘
- [ ] AlpaGasus: Training A Better Alpaca with Fewer Dataã€[paper](https://arxiv.org/abs/2307.08701)ã€‘ã€2023ã€‘
- [ ] Stanford alpaca: An instruction-following llama modelã€[code](https://github.com/tatsu-lab/stanford_alpaca#data-generation-process)ã€‘ã€2023ã€‘
- [ ] Instruction Tuning with GPT-4ã€[paper](https://arxiv.org/abs/2304.03277)ã€‘ã€2023ã€‘
- [ ] Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Qualityã€[paper](https://lmsys.org/blog/2023-03-30-vicuna/)ã€‘ã€2023ã€‘
- [ ] Falcon-40B: an open large language model with state-of-the-art performanceã€[code](https://huggingface.co/tiiuae)ã€‘ã€2023ã€‘
- [ ] OpenChat: Advancing Open-source Language Models with Imperfect Dataã€[code](https://github.com/imoneoi/openchat)ã€‘ã€2023ã€‘
- [ ] OpenAssistant Conversations -- Democratizing Large Language Model Alignmentã€[paper](https://arxiv.org/abs/2304.07327)ã€‘ã€2023ã€‘
- [ ] Training language models to follow instructions with human feedback(InstructGPT)ã€[paper](https://arxiv.org/abs/2203.02155)ã€‘ã€2022ã€‘
- [ ] Unnatural Instructions: Tuning Language Models with (Almost) No Human Laborã€[paper](https://arxiv.org/abs/2212.09689)ã€‘ã€2022ã€‘
- [ ] Self-Instruct: Aligning Language Models with Self-Generated Instructionsã€[paper](https://arxiv.org/abs/2212.10560)ã€‘ã€2022ã€‘


#### Agent
- [ ] The Rise and Potential of Large Language Model Based Agents: A Surveyã€[paper](https://arxiv.org/abs/2309.07864)ã€‘ã€[code](https://github.com/WooooDyy/LLM-Agent-Paper-List)ã€‘ã€2023ã€‘
- [ ] ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Modelsã€[paper](https://arxiv.org/abs/2309.00986)ã€‘ã€[code](https://github.com/modelscope/modelscope-agent)ã€‘ã€2023ã€‘


#### ç»¼è¿°
- [ ] Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Modelsã€[paper](http://arxiv.org/abs/2308.14149)ã€‘ã€[code](https://github.com/GPT-Alternatives/gpt_alternatives)ã€‘ã€2023ã€‘
