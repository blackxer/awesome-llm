# 论文资源

#### 大模型训练
- [ ] Gemma: Open Models Based on Gemini Research and Technology【[paper](https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf)】【2024】
- [ ] OLMo: Accelerating the Science of Language Models【[paper](https://arxiv.org/pdf/2402.00838.pdf)】【[code](https://github.com/allenai/OLMo)】【2024】
- [ ] 2x Faster Language Model Pre-training via Masked Structural Growth【[paper](https://arxiv.org/abs/2305.02869)】【2023】
- [ ] Freelm: Fine-tuning-free language model【[paper](https://arxiv.org/abs/2305.01616)】【2023】
- [ ] GPT-4 technical report【[paper](https://arxiv.org/abs/2303.08774)】【2023】
- [ ] Research without Re-search: Maximal Update Parametrization Yields Accurate Loss Prediction across Scales【[paper](https://arxiv.org/abs/2304.06875)】【[code](https://github.com/cofe-ai/Mu-scaling)】【2023】
- [x] FLM-101B: An Open LLM and How to Train It with $100K Budget【[paper](https://arxiv.org/pdf/2309.03852.pdf)】【[code](https://huggingface.co/CofeAI/FLM-101B)】【2023】
- [x] Baichuan2【[paper](https://cdn.baichuan-ai.com/paper/Baichuan2-technical-report.pdf)】【[code](https://github.com/baichuan-inc/Baichuan2)】【2023】
- [x] llama2 【[论文](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)】【[代码](https://github.com/facebookresearch/llama)】【2023】
- [x] llama【[论文](https://arxiv.org/pdf/2302.13971v1.pdf)】【[代码](https://github.com/facebookresearch/llama/tree/llama_v1)】【2023】
- [x] opt 训练日志编年史【[Chronicles ](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/chronicles/README.md)】【2022】
- [ ] Training Compute-Optimal Large Language Models【[paper](https://arxiv.org/abs/2203.15556)】【2022】
- [ ] Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer【[paper](https://arxiv.org/abs/2203.03466)】【[code](https://github.com/microsoft/mup)】【2022】
- [ ] Scaling Laws for Neural Language Models【[paper](https://arxiv.org/abs/2001.08361)】【2020】
- [ ] Scaling Laws for Autoregressive Generative Modeling【[paper](https://arxiv.org/abs/2010.14701)】【2020】


#### 对齐
- [ ]【[paper]()】【[code]()】【2024】
- [ ] [To Believe or Not to Believe Your LLM](https://arxiv.org/abs/2406.02543)【2024】
- [ ] Inverse-RLignment: Inverse Reinforcement Learning from Demonstrations for LLM Alignment【[paper](https://arxiv.org/pdf/2405.15624)】【2024】
- [ ] SimPO: Simple Preference Optimization with a Reference-Free Reward【[paper](https://arxiv.org/pdf/2405.14734)】【[code](https://github.com/princeton-nlp/SimPO)】【2024】
- [ ] Aligner : Achieving Efficient Alignment through Weak-to-Strong Correction【[paper](https://arxiv.org/abs/2402.02416)】【[code](https://aligner2024.github.io)】【2024】
- [ ] A Minimaximalist Approach to Reinforcement Learning from Human Feedback【[paper](https://arxiv.org/abs/2401.04056)】【2024】


#### 大模型里的小模型😂
- [ ] MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases【[paper](https://arxiv.org/pdf/2402.14905.pdf)】【2024】
- [ ] MiniCPM：Unveiling the Potential of End-side Large Language Models【[paper](https://shengdinghu.notion.site/MiniCPM-c805a17c5c8046398914e47f0542095a)】【[code](https://github.com/OpenBMB/MiniCPM)】【2024】
- [ ] SliceGPT: Compress Large Language Models by Deleting Rows and Columns【[paper](https://arxiv.org/pdf/2401.15024.pdf)】【2024】
- [ ] Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning【[paper](https://arxiv.org/abs/2310.06694)】【[code](https://xiamengzhou.github.io/sheared-llama/)】【2023】
- [ ] Textbooks Are All You Need II: phi-1.5 technical report【[paper](https://arxiv.org/abs/2309.05463)】【】【2023】
- [ ] TinyLlama-1.1B【[code](https://github.com/jzhang38/TinyLlama)】【2023】


#### 位置编码
- [ ] YaRN: Efficient Context Window Extension of Large Language Models【[paper](https://arxiv.org/abs/2309.00071)】【[code](https://github.com/jquesnelle/yarn)】【2023】
- [ ] A length-extrapolatable transformer【[paper](https://arxiv.org/pdf/2212.10554.pdf)】【[code](https://github.com/sunyt32/torchscale)】【2022】
- [ ] RoFormer: Enhanced Transformer with Rotary Position Embedding【[paper](https://arxiv.org/abs/2104.09864)】【[code](https://github.com/ZhuiyiTechnology/roformer)】【2021】
- [ ] Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation 【[paper](https://arxiv.org/abs/2108.12409)】【[code](https://github.com/ofirpress/attention_with_linear_biases)】【2021】


#### 轻量微调
- [ ] Memory Efficient Optimizers with 4-bit States【[paper](https://arxiv.org/abs/2309.01507)】【[code]( https://github.com/thu-ml/low-bit-optimizers)】【2023】
- [x] Stack More Layers Differently:High-Rank Training Through Low-Rank Updates (ReLoRA)【[paper](https://arxiv.org/abs/2307.05695)】【[code](https://github.com/guitaricet/peft_pretraining)】【2023】
- [x] QLORA: Efficient Finetuning of Quantized LLMs 【[论文](https://arxiv.org/pdf/2305.14314v1.pdf)】【[代码](https://github.com/artidoro/qlora)】【2023】
- [x] LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS 【[论文](https://arxiv.org/pdf/2106.09685.pdf)】【[代码](https://github.com/microsoft/LoRA)】【2021】
- [x] INTRINSIC DIMENSIONALITY EXPLAINS THE EFFECTIVENESS OF LANGUAGE MODEL FINE-TUNING 【[论文](https://arxiv.org/pdf/2012.13255.pdf)】【2020】
- [x] MEASURING THE INTRINSIC DIMENSION OF OBJECTIVE LANDSCAPES 【[论文](https://arxiv.org/pdf/1804.08838.pdf)】【2018】


#### 思维链推理
- [ ] [BoT: Buffer of Thoughts: Thought-Augmented Reasoning with Large Language Models](https://arxiv.org/abs/2406.04271)【2024】
- [ ] Meta-Prompting: Enhancing Language Models with Task-Agnostic Scaffolding【[paper](https://arxiv.org/abs/2401.12954)】【[code](https://github.com/suzgunmirac/meta-prompting)】【2024】
- [ ] Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4【[paper](https://arxiv.org/abs/2312.16171)】【[code](https://github.com/VILA-Lab/ATLAS)】【2023】
- [ ] Towards Better Chain-of-Thought Prompting Strategies: A Survey【[paper](https://browse.arxiv.org/abs/2310.04959)】【2023】
- [ ] Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models (AoT)【[paper](https://arxiv.org/pdf/2308.10379.pdf)】【】【2023】
- [ ] Large Language Models as Optimizers 【[paper](https://arxiv.org/abs/2309.03409)】【2023】
- [ ] Graph of Thoughts: Solving Elaborate Problems with Large Language Models (GoT)【[paper](https://arxiv.org/pdf/2308.09687v2.pdf)】【[code](https://github.com/spcl/graph-of-thoughts)】【2023】
- [x] Verify-and-Edit: A Knowledge-Enhanced Chain-of-Thought Framework【[paper](https://arxiv.org/abs/2305.03268)】【[code](https://github.com/RuochenZhao/Verify-and-Edit)】【2023】
- [x] Tree of thoughts: Deliberate problem solving with large language models (ToT)【[paper](https://arxiv.org/pdf/2305.10601.pdf)】【[code](https://github.com/princeton-nlp/tree-of-thought-llm)】【2023】
- [x] Large language model guided tree-of-thought (ToT)【[paper](https://arxiv.org/pdf/2305.08291.pdf)】【[code](https://github.com/jieyilong/tree-of-thought-puzzle-solver)】【2023】
- [x] React: Synergizing reasoning and acting in language models【[paper](https://arxiv.org/abs/2210.03629)】【[code](https://github.com/ysymyth/ReAct)】【2023】
- [x] Reflexion: an autonomous agent with dynamic memory and self-reflection【[paper](https://arxiv.org/pdf/2303.11366.pdf)】【[code](https://github.com/noahshinn024/reflexion)】【2023】
- [ ] Automatic prompt augmentation and selection with chain-of-thought from labeled data【2023】
- [x] Decomposition enhances reasoning via self-evaluation guided decoding【[paper](https://arxiv.org/pdf/2305.00633.pdf)】【[code](https://github.com/YuxiXie/SelfEval-Guided-Decoding)】【2023】
- [x] Self-consistency improves chain of thought reasoning in language models (CoT-SC)【[paper](https://arxiv.org/pdf/2203.11171.pdf)】【2022】
- [x] Chain of thought prompting elicits reasoning in large language models (CoT)【[paper](https://arxiv.org/pdf/2201.11903v6.pdf)】【2022】
- [ ] Automatic Chain of Thought Prompting in Large Language Models【[paper](https://arxiv.org/pdf/2210.03493.pdf)】【2022】
- [ ] LogiCoT: Logical Chain-of-Thought Instruction-Tuning【[paper](https://arxiv.org/pdf/2305.12147.pdf)】【2023】
- [ ] Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models【[paper](https://arxiv.org/pdf/2305.10276.pdf)】【2023】
- [ ] System 2 Attention (is something you might need too)【[paper](https://arxiv.org/pdf/2311.11829.pdf)】【2023】
- [ ] Thread of Thought Unraveling Chaotic Contexts【[paper](https://arxiv.org/abs/2311.08734)】【2023】
- [ ] Tab-CoT: Zero-shot Tabular Chain of Thought【[paper](https://arxiv.org/pdf/2305.17812.pdf)】【2023】
- [ ] Language Models are Few-Shot Learners【[paper](https://arxiv.org/abs/2005.14165v4)】【2020】


#### 指令数据集生成
- [ ] Aya Dataset: An Open-Access Collection for Multilingual Instruction Tuning【[paper](https://arxiv.org/abs/2402.06619)】【2024】
- [ ] LESS: Selecting Influential Data for Targeted Instruction Tuning【[paper](https://arxiv.org/abs/2402.04333)】【2024】
- [x] Large language models are human-level prompt engineers【[paper](https://arxiv.org/pdf/2211.01910.pdf)】【[code](https://github.com/keirp/automatic_prompt_engineer)】【2023】
- [x] Self-Alignment with Instruction Backtranslation【[paper](https://arxiv.org/abs/2308.06259)】【2023】
- [ ] WizardLM: Empowering Large Language Models to Follow Complex Instructions【[paper](https://arxiv.org/abs/2304.12244)】【2023】
- [ ] LongForm: Optimizing Instruction Tuning for Long Text Generation with Corpus Extraction【[paper](https://arxiv.org/abs/2304.08460)】【2023】
- [ ] Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision【[paper](https://arxiv.org/abs/2305.03047)】【2023】
- [ ] LIMA: Less Is More for Alignment【[paper](https://arxiv.org/abs/2305.11206)】【2023】
- [ ] AlpaGasus: Training A Better Alpaca with Fewer Data【[paper](https://arxiv.org/abs/2307.08701)】【2023】
- [ ] Stanford alpaca: An instruction-following llama model【[code](https://github.com/tatsu-lab/stanford_alpaca#data-generation-process)】【2023】
- [ ] Instruction Tuning with GPT-4【[paper](https://arxiv.org/abs/2304.03277)】【2023】
- [ ] Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality【[paper](https://lmsys.org/blog/2023-03-30-vicuna/)】【2023】
- [ ] Falcon-40B: an open large language model with state-of-the-art performance【[code](https://huggingface.co/tiiuae)】【2023】
- [ ] OpenChat: Advancing Open-source Language Models with Imperfect Data【[code](https://github.com/imoneoi/openchat)】【2023】
- [ ] OpenAssistant Conversations -- Democratizing Large Language Model Alignment【[paper](https://arxiv.org/abs/2304.07327)】【2023】
- [ ] Training language models to follow instructions with human feedback(InstructGPT)【[paper](https://arxiv.org/abs/2203.02155)】【2022】
- [ ] Unnatural Instructions: Tuning Language Models with (Almost) No Human Labor【[paper](https://arxiv.org/abs/2212.09689)】【2022】
- [ ] Self-Instruct: Aligning Language Models with Self-Generated Instructions【[paper](https://arxiv.org/abs/2212.10560)】【2022】


#### Agent
- [ ] Agents: An Open-source Framework for Autonomous Language Agents【[paper](https://arxiv.org/pdf/2309.07870.pdf)】【[code](https://github.com/aiwaves-cn/agents)】【2023】
- [ ] The Rise and Potential of Large Language Model Based Agents: A Survey【[paper](https://arxiv.org/abs/2309.07864)】【[code](https://github.com/WooooDyy/LLM-Agent-Paper-List)】【2023】
- [ ] ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models【[paper](https://arxiv.org/abs/2309.00986)】【[code](https://github.com/modelscope/modelscope-agent)】【2023】


#### 可解释性
- [ ] Explainability for Large Language Models: A Survey【[paper](https://arxiv.org/abs/2309.01029)】【[code](https://github.com/hy-zhao23/Explainability-for-Large-Language-Models)】【2023】
- [ ] Towards Monosemanticity: Decomposing Language Models With Dictionary Learning【[paper](https://transformer-circuits.pub/2023/monosemantic-features/index.html)】【2023】


#### Embedding
- [ ] Nomic Embed: Training a Reproducible Long Context Text Embedder【[paper](https://static.nomic.ai/reports/2024_Nomic_Embed_Text_Technical_Report.pdf)】【[code](https://github.com/nomic-ai/contrastors)】【2024】
- [ ] BGE M3-Embedding: Multi-Lingual, Multi-Functionality,Multi-Granularity Text Embeddings Through Self-Knowledge Distillation【[paper](https://arxiv.org/pdf/2402.03216.pdf)】【[code](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/BGE_M3)】【2024】
- [ ] Improving Text Embeddings with Large Language Models【[paper](https://arxiv.org/pdf/2401.00368.pdf)】【2024】
- [ ] Matryoshka Representation Learning 【[paper](https://arxiv.org/pdf/2205.13147.pdf)】【[code](https://github.com/RAIVNLab/MRL)】【2022】
- [ ] Retrieve Anything To Augment Large Language Models (BGE2)【[paper](https://arxiv.org/pdf/2310.07554.pdf)】【[code](https://github.com/FlagOpen/FlagEmbedding/tree/master/FlagEmbedding/llm_embedder)】【2023】


#### 多模态
- [ ]【[paper]()】【[code]()】【2024】
- [ ] World Model on Million-Length Video And Language With RingAttention【[paper](https://arxiv.org/abs/2402.08268)】【[code](https://github.com/LargeWorldModel/LWM)】【2024】
- [ ] Bunny【paper待更新】【[code](https://github.com/BAAI-DCAI/Bunny)】【2024】
- [ ] MoE-LLaVA: Mixture of Experts for Large Vision-Language Models【[paper](https://arxiv.org/abs/2401.15947)】【[code](https://github.com/PKU-YuanGroup/MoE-LLaVA)】【2024】
- [ ] Small Language Model Meets with Reinforced Vision Vocabulary【[paper](https://arxiv.org/abs/2401.12503)】【[code](https://github.com/Ucas-HaoranWei/Vary-toy)】【2024】
- [ ] Unified-IO 2: Scaling Autoregressive Multimodal Models with Vision, Language, Audio, and Action【[paper](https://arxiv.org/abs/2312.17172)】【[code](https://github.com/allenai/unified-io-2)】【2023】
- [ ] ImageBind: One Embedding Space To Bind Them All【[paper](http://arxiv.org/pdf/2305.05665.pdf)】【[code](http://github.com/facebookresearch/ImageBind)】【2023】
- [ ] LLaVA: Large Language and Vision Assistant【[paper](https://arxiv.org/abs/2310.03744)】【[code](https://github.com/haotian-liu/LLaVA)】【2023】
- [ ] MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens【[paper](https://browse.arxiv.org/pdf/2310.02239v1.pdf)】【[code](https://github.com/eric-ai-lab/MiniGPT-5)】【2023】
- [ ] Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization【[paper](https://arxiv.org/abs/2309.04669)】【[code](https://github.com/jy0205/LaVIT)】【2023】
- [ ] Learning Transferable Visual Models From Natural Language Supervision (clip)【[paper](https://arxiv.org/abs/2103.00020)】【[code](https://github.com/OpenAI/CLIP)】【2021】


#### 综述
- [ ] A Systematic Survey of Prompt Engineering in Large Language Models: Techniques and Applications【[paper](https://arxiv.org/abs/2402.07927)】【2024】
- [ ] Large Language Models: A Survey【[paper](https://arxiv.org/pdf/2402.06196.pdf)】【2024】
- [ ] MM-LLMs: Recent Advances in MultiModal Large Language Models【[paper](https://arxiv.org/abs/2401.13601)】【2024】
- [ ] The What, Why, and How of Context Length Extension Techniques in Large Language Models – A Detailed Survey【[paper](https://arxiv.org/pdf/2401.07872.pdf)】【2024】
- [ ] A Survey on Data Augmentation in Large Model Era【[paper](https://export.arxiv.org/abs/2401.15422)】【[code](https://github.com/MLGroup-JLU/LLM-data-aug-survey)】【2024】
- [ ] A Survey on Multimodal Large Language Models【[paper](https://arxiv.org/pdf/2306.13549)】【[code](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)】【2023】
- [ ] A Survey of Large Language Models【[paper](https://arxiv.org/abs/2303.18223)】[code](https://github.com/RUCAIBox/LLMSurvey)】【2023】
- [ ] Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models【[paper](http://arxiv.org/abs/2308.14149)】【[code](https://github.com/GPT-Alternatives/gpt_alternatives)】【2023】
